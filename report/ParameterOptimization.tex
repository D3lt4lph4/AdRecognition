\chapter{Parameter optimization}

We've seen that the results after the simple approach (random choice of the parameter [either usual value or complete random]) can give good result but can also give under expectation ones depending on the method used.

Thus we now try to optimize each parameter used for each methods in order to find the best value to use each time. On this section, the svm will be calculated using the function auto-train and a kfold segmentation of one to keep the sample intact and not gives them any advantages.

\section{SVM}

For the SVM we will use the auto train function given in the Opencv library. As for the stopping point of the algorithm i will use both the max iteration and the epsilon accuracy. But one should keep in mind that this could mean awful result depending on the input since the algorithm tries to approach a maximum and might not reach it as fast as wanted depanding on the start point.

\subsection{linear}

The first type of model used was the linear one. The stopping criteria was of either a 1000 iterations (low value to keep consistancy with the following chapter which will require more calculations) or 1 for epsilon.

Results :
\begin{itemize}
  \item Correct classification : 90.085\%;
  \item Wrong classification : 9.91501\%;
  \item False positive non ad : 5.09915\%;
  \item False positive ad : 4.81586\%.
\end{itemize}


\subsection{Polynomial}

For this one, the number of itertion was keep at 1000 because it was taking too much time.

Results :
\begin{itemize}
  \item Correct classification : 87.5354\%;
  \item Wrong classification : 12.4646\%;
  \item False positive non ad : 8.2153\%;
  \item False positive ad : 4.24929\%.
\end{itemize}

\subsection{Radial Basis Function}

We keep 1000 iterations.

Results :
\begin{itemize}
  \item Correct classification : 93.2011\%;
  \item Wrong classification : 6.79887\%;
  \item False positive non ad : 0.566572\%;
  \item False positive ad : 6.23229\%.
\end{itemize}

\subsection{Sigmoid}

Still a 1000 iterations

Results :
\begin{itemize}
  \item Correct classification : 88,9518\%;
  \item Wrong classification : 11,0482\%;
  \item False positive non ad : 4.53258\%;
  \item False positive ad : 6.51558\%.
\end{itemize}

\section{Neural Network}
For the neural network, the layers used are as follow :
  \begin{itemize}
    \item input layer, 1558, the number of point in the vector inputs;
    \item output layer, 2 outputs the number of class;
    \item hidden layer, 20 neurons;
  \end{itemize}

 For this model only the number of cell in the hidden layer will be optimized, not the number of hidden layer (doc here). And the data will be randomized before use. The result shown are with the best parameter each time.

  Results :
  \begin{itemize}
    \item Correct classification : 90.6516\%;
    \item Wrong classification : 9.34844\%;
    \item False positive non ad : 4.53258\%;
    \item False positive ad : 4.81586\%.
  \end{itemize}

%Here ut figure with the error
%Some explanation

\section{Random forest}
For the random forrest i only played with the following parameter as it's the same with more, only giving more calculations :
  \begin{itemize}
    \item number of variable randomly selected sqrt(n) (usually the best number);
  \end{itemize}
Results :
\begin{itemize}
  \item Correct classification : 94.9008\%;
  \item Wrong classification : 5.09915\%;
  \item False positive non ad : 3.39943\%;
  \item False positive ad : 1.69972\%.
\end{itemize}
