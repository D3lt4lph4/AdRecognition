\section{Priors and normalization}

In this section we will add the priors in the parameter and normalize the data to see how it can improve the result on the trained methods.

The priors are, for opencv, argements given to the training model to specify the classes repartition within the data, here 84\% of non-ads and 16\% of ads.

Normalizing the data can improve results depending on the method used. For instance for the svm it can help improving the results by "giving the same improtance to each point in the data vector". Meaning no value will be overpowerful over the others (range from -30 to 30 aginst range from -1 to 1).

\subsection{SVM}

Here we still use the autotrain method still with 2 folds. Again here the main goal is to test the effects of normalization and priors on the dataset, not the get the best parameter with a crossvalidation.

\subsubsection{linear}

The first type of model used was the linear one. The stopping criteria was of either a 1000000 iterations or 0.001 for epsilon.

Results :
\begin{itemize}
  \item Correct classification : 90.37\%;
  \item Wrong classification : 9.632\%;
  \item False positive non ad : 8.78\%;
  \item False positive ad : 0.85\%.
\end{itemize}

The final parameters were the following ones :
\begin{itemize}
  \item C : 0.1.
\end{itemize}

\subsubsection{Polynomial}

Results :
\begin{itemize}
  \item Correct classification : 88.67\%;
  \item Wrong classification : 11.33\%;
  \item False positive non ad : 10.2\%;
  \item False positive ad : 1.13\%.
\end{itemize}

The final parameters were the following ones :
\begin{itemize}
  \item degree : 0.07
  \item gamma : 0.00225;
  \item coef0 : 1.4;
  \item C : 321.5.
\end{itemize}


\subsubsection{Radial Basis Function}


Results :
\begin{itemize}
  \item Correct classification : 89.52\%;
  \item Wrong classification : 10.48\%;
  \item False positive non ad : 9.92\%;
  \item False positive ad : 0.57\%.
\end{itemize}

The final parameters were the following ones :
\begin{itemize}
  \item gamma : 0.00015;
  \item C : 62.5.
\end{itemize}

\subsubsection{Sigmoid}

Results :
\begin{itemize}
  \item Correct classification : 83.85\%;
  \item Wrong classification : 16.15\%;
  \item False positive non ad : 16.15\%;
  \item False positive ad : 0\%.
\end{itemize}

The final parameters were the following ones :
\begin{itemize}
  \item gamma : 0.00001;
  \item coef0 : 0.1;
  \item C : 0.1.
\end{itemize}

There is still no change for this model, the classification is still not working.

\subsection{Neural Network}


  Results :
  \begin{itemize}
    \item Correct classification : 88.67\%;
    \item Wrong classification : 11.33\%;
    \item False positive non ad : 6.23\%;
    \item False positive ad : 5.10\%.
  \end{itemize}

The final number of cells within the hidden layer was of 90;

  \begin{figure}[h]
   \centering
   \includegraphics[scale=0.5]{../data/images/NNPri.png}
   \caption{Class diagram}
  \end{figure}

\subsection{Random forest}

Results :
\begin{itemize}
  \item Correct classification : 87.25\%;
  \item Wrong classification : 12.75\%;
  \item False positive non ad : 12.46\%;
  \item False positive ad : 0.2833\%.
\end{itemize}

As we can see we get a bit of improvement on the false positive ads but on the other side we lose accuracy on the false positive non-ads 
\begin{figure}[h]
 \centering
 \includegraphics[scale=0.5]{../data/images/RFPri.png}
 \caption{Class diagram}
\end{figure}
500
