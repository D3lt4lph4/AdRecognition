\section{Priors and normalization}

In this section we will add the priors in the parameter and normalize the data to see how it can improve the result on the trained methods.

The priors are for opencv argement given to the training model to specify the classes repartition within the data, here 84\% of non ad and 16\% of ads.

Normalizing the data can improve results depending on the method used. For instance for the svm it helps improving the results by "giving the same improtance to each point in the data vector". Meaning no value will be overpowerful over the others (range from -30 to 30 aginst range from -1 to 1).

\subsection{SVM}

Here we still use the autotrain method but only with 2 folds. Again here the main goal is to test the effects of normalization and priors on the dataset, not the get the best parameter with a crossvalidation.


\subsubsection{linear}

The first type of model used was the linear one. The stopping criteria was of either a 100000 iterations or 1 for epsilon.

Results :
\begin{itemize}
  \item Correct classification : 91.78\%;
  \item Wrong classification : 8.22\%;
  \item False positive non ad : 7.08\%;
  \item False positive ad : 1.13\%.
\end{itemize}

The optimal parameters selected were :
\begin{itemize}
  \item C 0.1;
\end{itemize}

\subsubsection{Polynomial}

For this one, the number of itertion was reduced to 1000 because it was taking too much time.

Results :
\begin{itemize}
  \item Correct classification : 90.37\%;
  \item Wrong classification : 9.63\%;
  \item False positive non ad : 8.22\%;
  \item False positive ad : 1.42\%.
\end{itemize}

The optimal parameters selected were :
\begin{itemize}
  \item degree 0.490000;
  \item ceof0 274.400000;
  \item gamma 0.506250;
  \item C 2.500000;
  \item number of support vectors 189.
\end{itemize}


\subsubsection{Radial Basis Function}

Since the result on the first model were not good enough I tried an other one, the polynomial one. As shown after the results were much better.

Results :
\begin{itemize}
  \item Correct classification : 91.22\%;
  \item Wrong classification : 8.78\%;
  \item False positive non ad : 7.93\%;
  \item False positive ad : 0.85\%.
\end{itemize}

The optimal parameters selected were :
\begin{itemize}
  \item ceof0 0.100000;
  \item gamma 0.000150;
  \item C 62.500000;
  \item number of support vectors 416.
\end{itemize}


\subsubsection{Sigmoid}

Since the result on the first model were not good enough I tried an other one, the polynomial one. As shown after the results were much better.

Results :
\begin{itemize}
  \item Correct classification : 83.85\%;
  \item Wrong classification : 16.15\%;
  \item False positive non ad : 16.15\%;
  \item False positive ad : 0\%.
\end{itemize}

The optimal parameters selected were :
\begin{itemize}
  \item ceof0 0.100000;
  \item gamma 0.000010;
  \item C 0.100000;
  \item number of support vectors 382.
\end{itemize}

\subsection{Neural Network}
For the neural network, the layers used are as follow :
  \begin{itemize}
    \item input layer, 1558, the number of point in the vector inputs;
    \item output layer, 2 outputs the number of class;
    \item hidden layer;
  \end{itemize}

The final number of cells within the hidden layer was of 90;

  Results :
  \begin{itemize}
    \item Correct classification : 92.92\%;
    \item Wrong classification : 7.08\%;
    \item False positive non ad : 4.25\%;
    \item False positive ad : 2.83\%.
  \end{itemize}


\subsection{Random forest}
The parameters used for the random forest are the following :
  \begin{itemize}
    \item number of variable randomly selected sqrt(n) (usually the best number);
  \end{itemize}
Results :
\begin{itemize}
  \item Correct classification : 93.2\%;
  \item Wrong classification : 6.80\%;
  \item False positive non ad : 5.95\%;
  \item False positive ad : 0.85\%.
\end{itemize}

500
