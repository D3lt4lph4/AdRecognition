\section{Data Description}

The first step in any machine learning related work is to take a look at the data in order to carry out preprocessing over it.
The project will be carried with a set of data that "represents a set of possible advertisements on Internet pages". It is made of 2359 instances (1978 nonads and 381 ads) and the number of attributes for each instance is of 1558. Each feature represent a data from a web page ranging from height to url and is in the form of a double (even integer most of the time).\\

If one takes a closer look to the data itselfit can be found that each vector correspond to three numbers at the begining followed by a series of 0 and 1.

Since there are a lot probable useless values, one thing to do as pre-treatment of the data could be to apply the PCA in order to reduce the dimension of the vectors. This would have many advantages, the models can be trained only on the data that carries most of the information, thus it allows to reduced the time of training. On an other topic, memory is not a problem here due to the small size of the dataset, but storing a lot of zeros is memory consuming for no reason and can saturate extremily quickly the RAM of a pc (plus storage on database takes a lot more space) thus reduction can help improve performance and memory consuption. But here since the PCA would most likely not impact on the classifiers results and since memory is not an issue, the PCA will not be performed even if it could be a good following to this assigment.\\

The data is divided in the following way, the original data is pre-cut into two subset one of 353 samples for testing across all the models and the other of 2006 samples for training all of the models. The class are spread over both of the subset to keep the initial ratio of the first dataset (0.161509). Then depending on the training method evaluated, the training dataset can be split again into two smaller subset. The only data kept complitely constant is the testing set in order to have a reference point for the comparision of the efficiency of each model.
For each subset (apart from the testing one) the vectors will be shuffled. The shuffle is not required but could improved greatly the effectivness of some of the algorithms choosen, espacially for the neural network one.
