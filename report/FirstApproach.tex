\section{First approach}

This section describe the first approach with the different techniques, here no optimization will be carried out. Each parameter used will be selected either randomly or following the standards in use for the technique.

The trainSet was used for training the data, there was no further separation (i.e test and training).

\subsection{SVM}

For the svm only the Support Vector Classification will be used, but each kernel will be tested (cf opencv documentation).
We will select the parameter value as "in between value" to see what we can get, but since they are selected at random, the result might be very poor for this part.

\subsubsection{Linear}

The first type of model used was the linear one. The parameter C was set to 1. The results were the following :
\begin{itemize}
  \item Correct classification : 81.87\%;
  \item Wrong classification : 18.13\%;
  \item False positive non ad : 15.30\%;
  \item False positive ad : 2.83\%.
\end{itemize}

We can see that we get not so bad results with 81\% of good classification. Also we can notice that this classifier is good to find the ad without error since it only has 2.8\% of miss classification against 15\%. This is probably the kind of behaviour we would expect from a classifier in that case. No false positive on the ad and a higher number for the false positive on the non-ad to avoid masking content intended for the user.

\subsubsection{Polynomial}

Then the polynamial kernel was tested. Among the parameters to set, the following choices were made :
\begin{itemize}
  \item degree to 15;
  \item gamma to 1;
  \item coeff0 to 1;
\end{itemize}

The results were the following :

\begin{itemize}
  \item Correct classification : 34.56\%;
  \item Wrong classification : 65.44\%;
  \item False positive non ad : 13.31\%;
  \item False positive ad : 52.12\%.
\end{itemize}

As it is this classifier is clearly not good as we have a higher percentage of wrong classification rather than the good ones.

\subsubsection{Radial Basis Function}

For this classifier, gamma was set to 1.
Results :
\begin{itemize}
  \item Correct classification : 90.37\%;
  \item Wrong classification : 9.63\%;
  \item False positive non ad : 9.06\%;
  \item False positive ad : 0.57\%.
\end{itemize}

This is the best classifier so far, with almost no false positive on the ad.


\subsubsection{Sigmoid}

The last kernel for the svm is the sigmoid, the parameters were set as follow :
\begin{itemize}
  \item gamma to 1;
  \item coef0 to 1;
  \item c to 1;
\end{itemize}

It gives the following results :
\begin{itemize}
  \item Correct classification : 16.15\%;
  \item Wrong classification : 83.85\%;
  \item False positive non ad : 0\%;
  \item False positive ad : 83.85\%.
\end{itemize}

This is probably the worst classifier so far. The given pourcentage mean the it classify all the data ad no matter the input.

\subsection{Neural Network}

For the neural network, the layers used are as follow :
  \begin{itemize}
    \item input layer, 1558, the number of point in the vector inputs;
    \item output layer, 2 outputs, the number of class;
    \item hidden layer, 20 neurons;
  \end{itemize}

  There will be no optimization on the number of cells used within the hidden layer was selected to be between the number of input and output and not too big. (There seem to be more of a consensus rather than a real rule for the number of nodes).

  Results :
  \begin{itemize}
    \item Correct classification : 92.22\%;
    \item Wrong classification : 7.08\%;
    \item False positive non ad : 4.25\%;
    \item False positive ad : 2.83\%.
  \end{itemize}


\subsection{Random forest}

The parameters used for the random forest are the following :
  \begin{itemize}
      \item depth of the tree : 100,
      \item minimum sample required for a leaf to split : 20 (set to recommended value of opencv),
      \item number of trees within the forest : 100,
      \item number of variable randomly selected sqrt(n) (usually the best number);
  \end{itemize}

Results :
\begin{itemize}
  \item Correct classification : 90.93\%;
  \item Wrong classification : 9.07\%;
  \item False positive non ad : 8.50\%;
  \item False positive ad : 0.57\%.
\end{itemize}

As we can see we get also good results when it comes to identify the ad without any mistakes.
