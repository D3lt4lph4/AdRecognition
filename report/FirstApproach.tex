\chapter{First approach}

\section{Data separation}

Since no opotimization will be carried at this stage there is no use in splitting again the bigger subset for the training of the different models.

\section{SVM}

Since we don't want to get the svm to be one step ahead of the other methods we will not use the auto train method of the svm class. We will select the parameter value as "in between value" to see what we can get, but since they are selected at my appreciation, the result might be very poor for this part.
For the svm only the Support Vector Classification will be used, but each kernel will be tested.


\subsection{linear}

The first type of model used was the linear one. The parameter C was set to 1;

Results :
\begin{itemize}
  \item Correct classification : 81.8697\%;
  \item Wrong classification : 18.1303\%;
  \item False positive non ad : 2.83286\%;
  \item False positive ad : 15.2975\%.
\end{itemize}

We can see that we get not so bad results with 81\% of good classification. Also we can notice that this classifier is best to find the non-ad since it only has 2.8\% of miss classification against 15\%.

\subsection{Polynomial}
The parameter degree was set to 15, gamma to 1 and coeff0 to 1.

Results :
\begin{itemize}
  \item Correct classification : 34.5609\%;
  \item Wrong classification : 65.4391\%;
  \item False positive non ad : 52.1246\%;
  \item False positive ad : 13.3144\%.
\end{itemize}

This classifier seems to be worse than the previous, only 34.6\% of correctly cassified sample, but on the other hand it has less false positives on the ad class (which might not be what one would want).

\subsection{Radial Basis Function}

For this classifier, gamma was set to 1.
Results :
\begin{itemize}
  \item Correct classification : 90.3683\%;
  \item Wrong classification : 9.63173\%;
  \item False positive non ad : 0.566572\%;
  \item False positive ad : 9.06516\%.
\end{itemize}

This is the best classifier so far, with almost no false positive on the non ad. Still this may not be the classifier a website would want because it confuses non-ad for ad.


\subsection{Sigmoid}
gamma set to 1, coef0 set to 1 and c set to 1.
Results :
\begin{itemize}
  \item Correct classification : 16.1473\%;
  \item Wrong classification : 83.8527\%;
  \item False positive non ad : 83.8527\%;
  \item False positive ad : 0\%.
\end{itemize}

This is probably the worst classifier so far if we look at the number, yet if you want to be sure not mistake a non-ad for an ad this is the one to pick among the smv classifier.

\section{Neural Network}
For the neural network, the layers used are as follow :
  \begin{itemize}
    \item input layer, 1558, the number of point in the vector inputs;
    \item output layer, 2 outputs the number of class;
    \item hidden layer, 20 neurons;
  \end{itemize}
  There will be no optimization on the number of cells used within the hidden layer was selected to be between the number of input and output and not too big.

  Results :
  \begin{itemize}
    \item Correct classification : 91.2181\%;
    \item Wrong classification : 8.78187\%;
    \item False positive non ad : 3.96601\%;
    \item False positive ad : 4.81586\%.
  \end{itemize}


\section{Random forest}
The parameters used for the random forest are the following :
  \begin{itemize}
      \item depth of the tree : 100,
      \item minimum sample required for a leaf to split : 20 (set to recommended value of opencv),
      \item number of trees within the forest : 100,
      \item number of variable randomly selected sqrt(n) (usually the best number);
  \end{itemize}
  
Results :
\begin{itemize}
  \item Correct classification : 95.1841\%;
  \item Wrong classification : 4.81586\%;
  \item False positive non ad : 3.68272\%;
  \item False positive ad : 1.13314\%.
\end{itemize}
